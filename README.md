# Navigating the Labyrinth: Evaluating and Enhancing LLMsâ€™ Ability to Reason About Search Problems

[![DOI](https://zenodo.org/badge/814337764.svg)](https://zenodo.org/doi/10.5281/zenodo.11632275)

This repository includes SearchBench, a dataset  of state-based problems that require combinatorial search and optimization for resolution. It also includes prompting and inference methods to assess and improve the performance of LLMs on SearchBench. Additionally, we provide automated pipelines that can generate an arbitrary number of instances for each of the 11 problem types in SearchBench and also analyze the feasibility, correctness, and optimality of the solutions generated by LLMs.


## Requirements

To install python dependencies:

```setup
pip install -r requirements.txt
```

## Text-based Prompting Experiments

LLMs prompted 0-shot text and 4-shot Chain of Thought (CoT) text prompts, achieve the following performance on the SearchBench dataset (averaged across all instances in the benchmark):

| Model name         | 0-shot text | 4-shot CoT text|
| ------------------ |---------------- | ---------------- | 
| GPT4      |  Feasible:8.9% Correct:0.6% Optimal:0.2%    |  Feasible:10.8% Correct:1.4% Optimal:0.2% |      
| GPT3.5    |  Feasible:2.3% Correct:0.2% Optimal:0%    |  Feasible:4.2% Correct:0.1 Optimal:0 | 

To reproduce these results and evaluate the models on the SearchBench dataset, using text-based prompting methods, run inference_text.py with the appropriate arguments. For example the code below reproduces the results for GPT4 using 4-shot CoT text prompt:
```
python inference_text.py --prompting_method CoT_text --model_name gpt4 --problem_types All --print_stats True --path_to_searchbench SearchBench.jsonl --openai_key YOUR_OPENAI_KEY
```


For detailed information about the command-line argument choices of the scripts mentioned here, you can execute them with the '--help' option. For instance, to get a list of command-line argument choices of inference_text.py, run the following command:
```
python inference_text.py --help
```


## Code-based Prompting Experiments

LLMs prompted with the three code-based prompts used in our experiments achieve the following performance on the SearchBench dataset (averaged across all instances in the benchmark):

| Model name         | 0-shot code | 4-shot A* |  4-shot MSMT A* |
| ------------------ |---------------- | ---------------- | ---------------- | 
| GPT4      | Feasible:19.1% Correct:11.7% Optimal:4.7% |  Feasible:29.1% Correct:16.8% Optimal:9.7% | Feasible:64.6% Correct:51.7% Optimal:28.6% |     
| GPT3.5    | Feasible:3.1% Correct:1.4% Optimal:0% |  Feasible:7.9% Correct:2.8% Optimal:1.6% | Feasible:41.6% Correct:12.1% Optimal:0.8% |
| Code Llama   | Feasible:5.1% Correct:0.5% Optimal:0.5% |  Feasible:4.6% Correct:0.5% Optimal:0.2% | Feasible:8.0% Correct:0.1% Optimal:0.1% | 

To evaluate the models on the SearchBench dataset using the 0-shot code and A* prompting and reproduce the results, run inference_single_model_code.py with the appropriate arguments. For example the code below reproduces the results for GPT4 using A* prompt:

```
python inference_single_model_code.py --prompting_method A* --model_name gpt4 --execution_time_limit 3200 --problem_types All --print_stats True --path_to_searchbench SearchBench.jsonl --openai_key YOUR_OPENAI_KEY
```

To evaluate the models on the SearchBench dataset using the MSMT A* prompting, run inference_Astar_MSMT.py with the appropriate command-line arguments. For example the code below reproduces the results for GPT4 using MSMT A* prompt:

```
python inference_Astar_MSMT.py --model_name gpt4 --execution_time_limit 3200 --problem_types All --print_stats True --path_to_searchbench SearchBench.jsonl --openai_key YOUR_OPENAI_KEY
```

## Generating New Instances of the Problem Types

1. Navigate to the folder named after the problem type.
2. Modify the variables in the 'get_problems()' function within the 'dataset.py' file located in the same folder. This will allow you to adjust the range of difficulty for the new instances.
3. Then to develop the jsonl file of all new instances in a manner compatible with our inference pipelines and to obtain the optimal solution for each new instance, execute the script provided below.

```
python get_opt_solutions.py --Astar_execution_time_limit 600 --problem_type 8_puzzle --print_stats True --path_to_new_instances_jsonl new_SearchBench.jsonl  
```

This script should be run for each problem type to obtain the optimal solution for each instance. However, you can use the same 'new_SearchBench.jsonl' file to store all new instances across different problem types. To evaluate the models on the new benchmark with your custom range of difficulty and number of instances, provide the path to this file as the 'path_to_searchbench' command-line argument when running the inference scripts.
